{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "cmbHmh2f4d-G",
    "outputId": "eed9fbb6-c6f3-45d0-f423-945738ab67b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Collecting xformers==0.0.29.post3\n",
      "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
      "Collecting trl\n",
      "  Downloading trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
      "Collecting cut_cross_entropy\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting unsloth_zoo\n",
      "  Downloading unsloth_zoo-2025.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.19.1-py3-none-any.whl (376 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading unsloth_zoo-2025.7.2-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xformers, unsloth_zoo, trl, cut_cross_entropy, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.46.1 cut_cross_entropy-25.1.1 trl-0.19.1 unsloth_zoo-2025.7.2 xformers-0.0.29.post3\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.5)\n",
      "Collecting datasets>=3.4.1\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (0.70.15)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (3.11.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth-zoo 2025.7.2 requires msgspec, which is not installed.\n",
      "unsloth-zoo 2025.7.2 requires tyro, which is not installed.\n",
      "unsloth-zoo 2025.7.2 requires protobuf<4.0.0, but you have protobuf 5.29.5 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.0.0 fsspec-2025.3.0\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.7.2-py3-none-any.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m377.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth-2025.7.2-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.5/297.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unsloth\n",
      "Successfully installed unsloth-2025.7.2\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-8i7xs04s\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-8i7xs04s\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit df49b399dc02a2375f0e9bd0e74c544247ab3976\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.54.0.dev0-py3-none-any.whl size=11836103 sha256=6799a6c469e992126e9a304b113817088e29cbe9e712d8f2266dfb205225554c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ysrve9i4/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.1\n",
      "    Uninstalling transformers-4.53.1:\n",
      "      Successfully uninstalled transformers-4.53.1\n",
      "Successfully installed transformers-4.54.0.dev0\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.16)\n"
     ]
    }
   ],
   "source": [
    "#@title 1. Install Dependencies\n",
    "#@markdown This cell installs the necessary libraries from Unsloth and Hugging Face.\n",
    "# %%capture\n",
    "import os\n",
    "\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "!pip install --no-deps unsloth\n",
    "\n",
    "# Install latest transformers for Gemma 3N\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers.git\n",
    "!pip install --no-deps --upgrade timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581,
     "referenced_widgets": [
      "a38b32729c044e9dbdcf2d8257578213",
      "42a69fd5fc73446b83ee34d9cca6cf1e",
      "1ca416ac1e384fcaa93d07165487d910",
      "c8a5f21cd1744e3bb24965c1449a4167",
      "995f2692b35b48eb96c4ffd223388a01",
      "a671a95c8eaa418191e8ed5a6713d40b",
      "5914cfd0c14e461fac06bf09568ffcd0",
      "340fdca001db4d2bb0a82fd99706e23b",
      "54ad9213274a419aa2943585272d796d",
      "43676f1ebd5345b7a548cdcc3566d5d7",
      "00b3dfaef3ea4dbaa12c167a1760bbb7",
      "40d9945a525b46ebaba7a13380fa6b79",
      "43dc7d6c58824e5db47b0cbd4bf1c7c6",
      "408e1cecbdd147d795b9ad03b534b279",
      "372e846205c54e0f9ea791bbd6885745",
      "b5a83aa332fc4137bebc4048fc50cf8f",
      "7fc39746f72c49d4b65f0768ba0c22e6",
      "b6e0a0e803ec465f878935bbc563fe37",
      "3bcc524a4a7c435b887e40f37821289a",
      "39525b7097a149d5ad6b27846c6fa069",
      "b99072df2c5f4e8b8b02f6eadafbb0c9",
      "6aa2b06040eb46b3a29717c8f9a398f4",
      "d893272621334f90b2c17cabd8a97c64",
      "2f77e87cb27b4242811252f1bc714adb",
      "bcf26ee7b3ee404f87f4ca29f8a19786",
      "1eb54d13392041f28c5ae0d8a5ea4e78",
      "9cf12efd7cdc4f0da93a60d354669507",
      "1bc5875bc3614a95ae7f376dac809333",
      "d91b4093c42f419a90f0d41b2ee9431a",
      "afe4621185d24b37ba50ff8023cafccd",
      "9fb1cbe1654049df89109aeb1b29ede5",
      "b0913f37f2a0430a8c02e3d20866a5cb",
      "4153511396764ce98f93e326da340d57",
      "783e2c77095443f3b18751367225b419",
      "ae860aaa95ba41f784a93a8801f14b31",
      "c3c503885dd9434bb45a5450e116f2c0",
      "4a9027a00c504be2b5d071b3bfbcabe0",
      "ba859ec588a8445996cbec2b2351a5e7",
      "38c204f79ffe40c19019da394c9b9db3",
      "8a3d97dba6554f4ca6bb01a4229a568d",
      "b8a2d386004146e084e73f0d5947c5fe",
      "a5b019939c1f4ebc82353c8b41c26d3a",
      "63cd3dce7037470997fb5dffb5a96827",
      "fb888a3e05ef468a951882cdde63b0b6",
      "b8a99a4002224274ae991cfa73c1c68e",
      "865b9ce3fd6940c8bb097870877d7c54",
      "a5ed2722d89e4d5f9abb896cfc04d8cb",
      "c566ec9b351d4dee93b8b3a866fa2964",
      "cecc8468b0954074ba43200de93d7515",
      "043610162d854922b822cc12254bae21",
      "eac30440bf7648d3ba27636acdfd2ac9",
      "afb982063c4c4294a735d101e8597a5c",
      "6a623415f3a643f5a893d074855a65b4",
      "0abaa29d870640c080c1e0a29f92fbee",
      "41afbaf42e6947fbaf338cbde7f3b74c",
      "0c32998742b8469a822229780ac84ffc",
      "22dad52000e641b0ae4f9148bffaa6a0",
      "8d940dae827148acaae71ea64039221c",
      "a47c124883324afb96893e759f7c2ed7",
      "31e63ecfb240434c97544064b5cc673c",
      "af07c2a747bb4088bd619a5c4a83f350",
      "95047e0583654288a0f5980efe6af78b",
      "8bbda88b379940ffa523862f97dc7189",
      "793fb16e88404cd2afee518443532939",
      "31688849eb614aa1bda9662ac19066db",
      "154fef9e0b23464b877faafa5368e5ac",
      "2534ebbcbbaf4cf5a1f69ef27d1e6656",
      "e4ca184a5b724fecb869d2c4384268ce",
      "825a5b2709e34ae1ba1694f1f9796121",
      "dc21fb7506a64c91aa10af0c09717c17",
      "77a4515ad5cf4670b821ef9d244ebd9e",
      "231411198aff4d938bd1e435b75b2ef6",
      "186e264244e24cabaead5190ff821d7e",
      "55a6b0f8202a4fbfa007a3a00eeff294",
      "26a1ac4175b64609adaa083116ef9175",
      "2419727b3405449197a62e4c8e59aacf",
      "f2bef1cff1294a80be29752cf022f6a4",
      "936d7e41242f4a6aa3a528af7beda057",
      "d0499922b2294ee591fdbbdf6c46f821",
      "109b7dc9834144eb96943d7acbf1cddb",
      "05bf8987505b40cfad21942cd09c1b71",
      "5c8f9dc38a274e9ca32966e5570edd94",
      "2df4485b0d5b44e4aa5de19b941eff84",
      "75620cdc0b5a4d4ebaf7550e9647fb60",
      "e031cc0b905249f8ae2641050b6fef92",
      "1b95c28f170248329561718005dcbe15",
      "3c798c2618124339ae10d15cc315e87b",
      "bacc563eaee64f70ad23b47f06363a80",
      "c8797a24dbf14f7fab672a005aa4eaf6",
      "e709435dcad04f809395e690050c220f",
      "8c36de6117c84009ab68465147847c7e",
      "7d1d578dd9344161a31a20220d5e577e",
      "75ca291a604f481d8aa4d3b8b1d8a5a3",
      "4c82981b6593442380e5f2f40cae1c56",
      "726c0c9e1be94adb91c009701c124d67",
      "763bc5376bbc4a0d9a8cc71532adfd05",
      "b4adc857dae94050a9c963eca4482f2b",
      "707231624dba4a5d802d15270b9c7e32",
      "f1fac0ec56e34d82a2d21f9af7b1076d",
      "de751bbd1f0944c88d56fed85de4e671",
      "49cd97a065894a4b9ad6e9e84873908a",
      "2f8da508e2504ebb9bba1d98f919962c",
      "c17dbf860550430aab11101838ec6e53",
      "7ceaef29a8b643d3b1d37cf3c7c786a1",
      "faa0e8c7d70841669929ddd4d1c96533",
      "85b281387d744fabad170aafadcc0c6a",
      "58cc2d699d014dd18657a32753629ccf",
      "2886a5bde41b4a1ea1fc119ad7e3dbf6",
      "4501f092d4e042eb951bbda1e8a210df",
      "f9302196724141ba99923b8eb03f4413",
      "564cc24341664c42914ffe3c8cb5a26c",
      "41091ac88c68458d9d4af459074eab79",
      "0c662ae707f840a980b237d07f1c9cf4",
      "7803cead055842fb8a8bcff335813ec0",
      "03aa8c46161b4552a92a8c0669ea091d",
      "55c2a71a52394aa7859102e920edf675",
      "e2e6f968c75e44058257d4c899884b5c",
      "25edd40f79624744a5f5ec824e9b1f9c",
      "63da9b264af24621bfd37fea31f6bbb5",
      "e6415814b52844f09ffe9e4a3faed177",
      "58ba87305dd3439fbde10dddc33d2067",
      "8aadd14363374af08b1b289f47c209e0",
      "9c5923c54b3e4ffb8314b33839a44fc9",
      "3b9f82d52b864cf1aa1929f03d7c4e0e",
      "0072b4bb88144bae96789eb4565f29ca",
      "2e47bdd6ba7f4f68a8dd8ce20e14ac3f",
      "83af5171c120497d8e7781388f1754de",
      "b9e7e8ea32534f76b1c301050fa07663",
      "fa9fb42f713d49ab952c382f7d1360ac",
      "366ef06bd9414d74b4cdd379e879ffa5",
      "cb6996d58c814636b5f4b5e670bb0b7a",
      "7acbf74825f342b082ae43c9cd42c928",
      "3d472eb37ca5488589ce05b351811b9e",
      "2ecfef820b0741689a669306f833f051",
      "15fc3981a0d9439ba19e8242728f7cea",
      "3dcbbb64da2d451eb55e14dd56bc7c77",
      "b9f0a10da63e4233bd25b0400fcce1c2",
      "30959db6f4e84b76b87eaee05094edd6",
      "934bdfba8afa4dbfab5ee06e00ba74ac",
      "0f161aab4a7c462fb12d3aeb0ddf5d3f",
      "9986cf88d3bd475f9ed5dbf956ef396b",
      "a4a7a421ace241aba0d9fdf67399131a",
      "faa44e05724d43c895118f1789eb611a"
     ]
    },
    "id": "IgcMERzB4hA5",
    "outputId": "bf0cb54a-84e2-4d03-ee8b-d2da70f1ccce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.7.2: Fast Gemma3N patching. Transformers: 4.54.0.dev0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3N does not support SDPA - switching to eager!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38b32729c044e9dbdcf2d8257578213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d9945a525b46ebaba7a13380fa6b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d893272621334f90b2c17cabd8a97c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783e2c77095443f3b18751367225b419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a99a4002224274ae991cfa73c1c68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c32998742b8469a822229780ac84ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2534ebbcbbaf4cf5a1f69ef27d1e6656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936d7e41242f4a6aa3a528af7beda057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8797a24dbf14f7fab672a005aa4eaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de751bbd1f0944c88d56fed85de4e671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564cc24341664c42914ffe3c8cb5a26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aadd14363374af08b1b289f47c209e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d472eb37ca5488589ce05b351811b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title 2. Load Model and Tokenizer\n",
    "#@markdown This cell loads the pre-trained Gemma 3N model with 4-bit quantization for efficient inference.\n",
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer from Unsloth's Hugging Face repository\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\", # A 4-bit quantized model\n",
    "    dtype = None,                 # None for auto detection\n",
    "    max_seq_length = 2048,        # Max sequence length for context\n",
    "    load_in_4bit = True,          # 4bit for efficiency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehKvo7Bk4pPq"
   },
   "outputs": [],
   "source": [
    "#@title 3. Define the Inference Function\n",
    "#@markdown This helper function will handle the process of generating a response from the model.\n",
    "#@markdown It takes the formatted prompt, generates text, and returns the clean output.\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "def run_inference(model, tokenizer, messages, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Runs inference on the model and returns the decoded text output.\n",
    "    \"\"\"\n",
    "    # Apply the chat template and tokenize\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True, # for generation\n",
    "        tokenize = True,\n",
    "        return_tensors = \"pt\",\n",
    "        return_dict = True,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate the output\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        temperature = 1.0,\n",
    "        top_p = 0.95,\n",
    "        top_k = 64,\n",
    "        use_cache = True,\n",
    "    )\n",
    "\n",
    "    # Decode and clean the output\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Clean the prompt from the output string to get only the generation\n",
    "    prompt_length = len(tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)[0])\n",
    "    clean_output = decoded_output[prompt_length:].strip()\n",
    "\n",
    "    return clean_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFZZ8Fk04sF2",
    "outputId": "c44725db-6f4f-4c37-df9f-4583bc8157f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ STEP 1: Analyzing the meme...\n",
      "\n",
      "📝 MODEL ANALYSIS:\n",
      " The image displays a map of the Mediterranean region, showing continents, oceans, and countries. The text \"What separates humans from animals?\" is superimposed on this geographical context. The juxtaposition of a map depicting human-defined geographical boundaries with a question about the difference between humans and animals is unsettling. It implicitly suggests a separation or distinction between humans and the animal kingdom, potentially reinforcing a human-centric view of the world. While not overtly biased, the combination of visual and textual elements could subtly promote a hierarchical view where humans are seen as fundamentally different from and superior to animals.\n",
      "\n",
      "✅ STEP 2: Determining if the content is hateful...\n",
      "\n",
      "🏁 FINAL DECISION:\n",
      " not hateful; While the meme's implications are unsettling and potentially promote a human-centric view, it doesn't directly target specific groups, use harmful stereotypes\n"
     ]
    }
   ],
   "source": [
    "#@title 4. Perform Zero-Shot Evaluation\n",
    "\n",
    "# ==========================================================================================\n",
    "# == USER INPUT: the path to meme image and the text content of the meme. ==\n",
    "# ==========================================================================================\n",
    "meme_image_path = \"/content/1.png\" #@param {type:\"string\"}\n",
    "meme_text = \"Atlantic Ocean| EUROPE Mediterranean Sea AFRICA • Encyclopaedia Britannica, Inc. What separates humans from animals? Black Sea ASIA\" #@param {type:\"string\"}\n",
    "# ==========================================================================================\n",
    "\n",
    "\n",
    "# STEP 1: Analyze the meme with the multi-modal prompt (Image + Text)\n",
    "print(\" STEP 1: Analyzing the meme...\")\n",
    "\n",
    "analysis_prompt = f\"\"\"Analyze this meme by considering\n",
    "both the visual elements and the text content:\n",
    "\"{meme_text}\"\n",
    "Focus on:\n",
    "1. What is shown in the image\n",
    "2. How the text relates to the image\n",
    "3. The overall message or intent\n",
    "4. Whether this combination promotes bias,\n",
    "discrimination, or harmful stereotypes\n",
    "Provide your analysis in a brief paragraph.\"\"\"\n",
    "\n",
    "# Formatting the multi-modal input for the model\n",
    "analysis_messages = [{\n",
    "    \"role\" : \"user\",\n",
    "    \"content\": [\n",
    "        { \"type\": \"image\", \"image\" : meme_image_path },\n",
    "        { \"type\": \"text\",  \"text\" : analysis_prompt }\n",
    "    ]\n",
    "}]\n",
    "\n",
    "# Getting the model's analysis\n",
    "analysis = run_inference(model, tokenizer, analysis_messages)\n",
    "print(\"\\n MODEL ANALYSIS:\\n\", analysis)\n",
    "\n",
    "\n",
    "# STEP 2: Make a decision based on the analysis (Text-only)\n",
    "print(\"\\n STEP 2: Determining if the content is hateful...\")\n",
    "\n",
    "decision_prompt = f\"\"\"Based on the following analysis\n",
    "of a meme:\n",
    "{analysis}\n",
    "Determine if this meme contains hateful content by\n",
    "considering:\n",
    "- Does it promote discrimination or prejudice?\n",
    "- Does it target specific groups based on protected\n",
    "characteristics?\n",
    "- Does it use harmful stereotypes or derogatory\n",
    "language?\n",
    "- Is the combination of image and text intended to\n",
    "cause harm or offense?\n",
    "Respond with ONLY'hateful' or 'not hateful', followed\n",
    "by a brief explanation after a semicolon.\"\"\"\n",
    "\n",
    "# Formating the text input for the model\n",
    "decision_messages = [{\n",
    "    \"role\" : \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : decision_prompt\n",
    "    }]\n",
    "}]\n",
    "\n",
    "# Getting the final decision from the model\n",
    "decision = run_inference(model, tokenizer, decision_messages, max_new_tokens=32)\n",
    "print(\"\\n FINAL DECISION:\\n\", decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hE3pPiD5FLGs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvDjZxY0FLPx",
    "outputId": "fe6ee423-14c1-4429-f3ea-6fc8e4df5dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ STEP 1: Analyzing the meme...\n",
      "\n",
      "📝 MODEL ANALYSIS:\n",
      " ## Analysis of the Meme\n",
      "\n",
      "The image shows a map of the Mediterranean Sea, highlighting the geographical division between Europe and Africa, and also showing the proximity of Asia. The text \"What separates humans from animals?\" juxtaposed with this geographical map is inherently misleading and promotes a harmful stereotype. It attempts to link the concept of \"humans\" with a specific geographical region (Europe) and implicitly suggests a separation or superiority over \"animals,\" which are not visually represented. This combination doesn't offer a meaningful answer to the question but rather reinforces a potentially biased and discriminatory view that places humans in a distinct and potentially superior category compared to the animal kingdom. The intent is likely to provoke thought or perhaps humor, but it falls short by relying on a geographically narrow and ultimately flawed premise to address a complex philosophical question.\n",
      "\n",
      "✅ STEP 2: Determining if the content is hateful...\n",
      "\n",
      "🏁 FINAL DECISION:\n",
      " not hateful; While the meme promotes a flawed premise and potentially biased views, it doesn't directly target specific groups with hate speech or derogatory language. It relies\n"
     ]
    }
   ],
   "source": [
    "#@title 4. Perform Zero-Shot Evaluation\n",
    "\n",
    "# ==========================================================================================\n",
    "# == USER INPUT: the path to meme image and the text content of the meme. ==\n",
    "# ==========================================================================================\n",
    "meme_image_path = \"/content/1.png\" #@param {type:\"string\"}\n",
    "meme_text = \"Atlantic Ocean| EUROPE Mediterranean Sea AFRICA • Encyclopaedia Britannica, Inc. What separates humans from animals? Black Sea ASIA\" #@param {type:\"string\"}\n",
    "# ==========================================================================================\n",
    "\n",
    "\n",
    "# STEP 1: Analyze the meme with the multi-modal prompt (Image + Text)\n",
    "print(\" STEP 1: Analyzing the meme...\")\n",
    "\n",
    "analysis_prompt = f\"\"\"Analyze this meme by considering\n",
    "both the visual elements and the text content:\n",
    "\"{meme_text}\"\n",
    "Focus on:\n",
    "1. What is shown in the image\n",
    "2. How the text relates to the image\n",
    "3. The overall message or intent\n",
    "4. Whether this combination promotes bias,\n",
    "discrimination, or harmful stereotypes\n",
    "Provide your analysis in a brief paragraph.\"\"\"\n",
    "\n",
    "# Formatting the multi-modal input for the model\n",
    "analysis_messages = [{\n",
    "    \"role\" : \"user\",\n",
    "    \"content\": [\n",
    "        { \"type\": \"image\", \"image\" : meme_image_path },\n",
    "        { \"type\": \"text\",  \"text\" : analysis_prompt }\n",
    "    ]\n",
    "}]\n",
    "\n",
    "# Getting the model's analysis\n",
    "analysis = run_inference(model, tokenizer, analysis_messages)\n",
    "print(\"\\n MODEL ANALYSIS:\\n\", analysis)\n",
    "\n",
    "\n",
    "# STEP 2: Make a decision based on the analysis (Text-only)\n",
    "print(\"\\n STEP 2: Determining if the content is hateful...\")\n",
    "\n",
    "decision_prompt = f\"\"\"Based on the following analysis\n",
    "of a meme:\n",
    "{analysis}\n",
    "Determine if this meme contains hateful content by\n",
    "considering:\n",
    "- Does it promote discrimination or prejudice?\n",
    "- Does it target specific groups based on protected\n",
    "characteristics?\n",
    "- Does it use harmful stereotypes or derogatory\n",
    "language?\n",
    "- Is the combination of image and text intended to\n",
    "cause harm or offense?\n",
    "Respond with ONLY'hateful' or 'not hateful', followed\n",
    "by a brief explanation after a semicolon.\"\"\"\n",
    "\n",
    "# Formating the text input for the model\n",
    "decision_messages = [{\n",
    "    \"role\" : \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : decision_prompt\n",
    "    }]\n",
    "}]\n",
    "\n",
    "# Getting the final decision from the model\n",
    "decision = run_inference(model, tokenizer, decision_messages, max_new_tokens=32)\n",
    "print(\"\\n FINAL DECISION:\\n\", decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSvEF1nQFWUH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
